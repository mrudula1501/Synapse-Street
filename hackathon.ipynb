{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8875833,"sourceType":"datasetVersion","datasetId":1421681}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:35:43.248802Z","iopub.execute_input":"2025-11-08T20:35:43.248981Z","iopub.status.idle":"2025-11-08T20:35:44.897000Z","shell.execute_reply.started":"2025-11-08T20:35:43.248965Z","shell.execute_reply":"2025-11-08T20:35:44.896055Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip -q install pandas numpy scikit-learn ta qdrant-client[fastembed] langgraph langchain-core langchain-community openai plotly\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:35:44.901393Z","iopub.execute_input":"2025-11-08T20:35:44.901660Z","iopub.status.idle":"2025-11-08T20:36:00.951820Z","shell.execute_reply.started":"2025-11-08T20:35:44.901631Z","shell.execute_reply":"2025-11-08T20:36:00.950766Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.2/471.2 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.4 which is incompatible.\nlangchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Load CSVs","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\n\nDATA = Path(\"/kaggle/input/us-stock-market-history-data-csv\")\n\ndf_hist = pd.read_csv(DATA/\"history.csv\", low_memory=False)\ndf_div  = pd.read_csv(DATA/\"dividends.csv\", low_memory=False)\ndf_split= pd.read_csv(DATA/\"splits.csv\", low_memory=False)\n\n# normalize columns\ndf_hist.columns  = [c.lower() for c in df_hist.columns]\ndf_div.columns   = [c.lower() for c in df_div.columns]\ndf_split.columns = [c.lower() for c in df_split.columns]\n\n# expected columns\n# history: date, ticker/symbol, open, high, low, close, adj_close?, volume\ndate_col = \"date\" if \"date\" in df_hist.columns else (\"timestamp\" if \"timestamp\" in df_hist.columns else \"datetime\")\ntic_col  = \"ticker\" if \"ticker\" in df_hist.columns else (\"symbol\" if \"symbol\" in df_hist.columns else None)\nassert tic_col is not None, \"No ticker/symbol column in history.csv\"\n\ndf_hist[date_col] = pd.to_datetime(df_hist[date_col])\ndf_hist = df_hist.rename(columns={date_col:\"date\", tic_col:\"ticker\"})\nkeep_hist = [c for c in [\"date\",\"ticker\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"] if c in df_hist.columns]\ndf_hist = df_hist[keep_hist].dropna(subset=[\"close\"]).sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n\n# dividends: date, ticker/symbol, dividend(s)\ndv_t = \"ticker\" if \"ticker\" in df_div.columns else (\"symbol\" if \"symbol\" in df_div.columns else None)\ndv_d = \"date\" if \"date\" in df_div.columns else (\"timestamp\" if \"timestamp\" in df_div.columns else None)\ndv_v = \"dividends\" if \"dividends\" in df_div.columns else (\"dividend\" if \"dividend\" in df_div.columns else None)\n\nif dv_t and dv_d and dv_v:\n    df_div = df_div.rename(columns={dv_t:\"ticker\", dv_d:\"date\", dv_v:\"dividend\"})\n    df_div[\"date\"] = pd.to_datetime(df_div[\"date\"])\n    df_div = df_div[[\"ticker\",\"date\",\"dividend\"]]\nelse:\n    df_div = pd.DataFrame(columns=[\"ticker\",\"date\",\"dividend\"])\n\n# splits: date, ticker/symbol, split_ratio OR numerator/denominator\nsp_t = \"ticker\" if \"ticker\" in df_split.columns else (\"symbol\" if \"symbol\" in df_split.columns else None)\nsp_d = \"date\" if \"date\" in df_split.columns else (\"timestamp\" if \"timestamp\" in df_split.columns else None)\n\n# try to derive a split ratio as float (e.g., 2-for-1 => 2.0)\nratio_col = None\nfor cand in [\"split_ratio\",\"ratio\",\"stock split\",\"split\"]:\n    if cand in df_split.columns:\n        ratio_col = cand\n        break\n\ndf_split = df_split.rename(columns={sp_t:\"ticker\", sp_d:\"date\"})\ndf_split[\"date\"] = pd.to_datetime(df_split[\"date\"])\n\ndef _parse_ratio(x):\n    # handles \"2:1\" / \"3/2\" / 2 / 0.5 …\n    if pd.isna(x): return 1.0\n    if isinstance(x,(int,float)): return float(x) if x>0 else 1.0\n    s = str(x).strip().replace(\" \", \"\")\n    for sep in [\":\",\"/\"]:\n        if sep in s:\n            a,b = s.split(sep,1)\n            try:\n                a,b = float(a), float(b)\n                return a/b if b!=0 else 1.0\n            except: return 1.0\n    try:\n        v = float(s)\n        return v if v>0 else 1.0\n    except:\n        return 1.0\n\nif ratio_col is None:\n    # if not available, assume no splits\n    df_split[\"ratio\"] = 1.0\nelse:\n    df_split[\"ratio\"] = df_split[ratio_col].apply(_parse_ratio)\n\ndf_split = df_split[[\"ticker\",\"date\",\"ratio\"]].dropna()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T02:41:33.148687Z","iopub.execute_input":"2025-11-09T02:41:33.148966Z","iopub.status.idle":"2025-11-09T02:42:05.055157Z","shell.execute_reply.started":"2025-11-09T02:41:33.148944Z","shell.execute_reply":"2025-11-09T02:42:05.054359Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Apply split adjustments to OHLCV","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Ensure sorted & light dtypes\ndf_hist = df_hist.sort_values([\"ticker\",\"date\"]).copy()\ndf_hist[\"ticker\"] = df_hist[\"ticker\"].astype(\"category\")\ndf_split = df_split.sort_values([\"ticker\",\"date\"]).copy()\ndf_split[\"ticker\"] = df_split[\"ticker\"].astype(\"category\")\n\n# 1) Attach a per-day split ratio to every history row (1.0 when no event)\ndf_hist = df_hist.merge(\n    df_split[[\"ticker\",\"date\",\"ratio\"]],\n    on=[\"ticker\",\"date\"],\n    how=\"left\"\n)\ndf_hist[\"ratio\"] = df_hist[\"ratio\"].fillna(1.0)\n\n# 2) Compute the \"future factor\" as reverse cumulative product per ticker\n#    (product of ratios from today to the end → back-adjust earlier prices)\ndf_hist[\"split_factor\"] = (\n    df_hist\n    .groupby(\"ticker\", sort=False)[\"ratio\"]\n    .transform(lambda s: s[::-1].cumprod()[::-1])\n)\n\n# 3) Adjust OHLC and volume (prices ÷ factor, volume × factor)\nfor c in [\"open\",\"high\",\"low\",\"close\"]:\n    if c in df_hist.columns:\n        df_hist[c+\"_adj\"] = df_hist[c] / df_hist[\"split_factor\"]\n\nif \"volume\" in df_hist.columns:\n    df_hist[\"volume_adj\"] = df_hist[\"volume\"] * df_hist[\"split_factor\"]\n\n# 4) Prefer adjusted columns; fallback to raw if missing\ncols_core = [\"date\",\"ticker\",\"open_adj\",\"high_adj\",\"low_adj\",\"close_adj\",\"volume_adj\"]\nfor c in cols_core:\n    if c not in df_hist.columns:\n        base = c.replace(\"_adj\",\"\")\n        df_hist[c] = df_hist.get(base, pd.NA)\n\ndf = (\n    df_hist[cols_core]\n    .sort_values([\"ticker\",\"date\"])\n    .dropna(subset=[\"close_adj\",\"volume_adj\"])\n    .reset_index(drop=True)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T02:42:24.882622Z","iopub.execute_input":"2025-11-09T02:42:24.883121Z","iopub.status.idle":"2025-11-09T02:42:56.200911Z","shell.execute_reply.started":"2025-11-09T02:42:24.883095Z","shell.execute_reply":"2025-11-09T02:42:56.200298Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Add dividends (ex-div flag + trailing yield)","metadata":{}},{"cell_type":"code","source":"# Merge dividends to daily rows; create ex-div flag and trailing 365d sum / price as yield proxy\nif not df_div.empty:\n    df = df.merge(df_div, on=[\"ticker\",\"date\"], how=\"left\")\n    df[\"dividend\"] = df[\"dividend\"].fillna(0.0)\n    df[\"ex_div_flag\"] = (df[\"dividend\"] > 0).astype(int)\n    df[\"div_roll_365\"] = (df.groupby(\"ticker\")[\"dividend\"]\n                            .rolling(365, min_periods=1).sum()\n                            .reset_index(level=0, drop=True))\n    df[\"div_yield_proxy\"] = df[\"div_roll_365\"] / df[\"close_adj\"].replace(0, pd.NA)\n    df = df.drop(columns=[\"div_roll_365\"])\nelse:\n    df[\"dividend\"] = 0.0\n    df[\"ex_div_flag\"] = 0\n    df[\"div_yield_proxy\"] = pd.NA\n\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T02:43:00.194465Z","iopub.execute_input":"2025-11-09T02:43:00.194734Z","iopub.status.idle":"2025-11-09T02:43:24.917444Z","shell.execute_reply.started":"2025-11-09T02:43:00.194713Z","shell.execute_reply":"2025-11-09T02:43:24.916719Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"        date ticker   open_adj   high_adj    low_adj  close_adj  volume_adj  \\\n0 1999-11-18      A  32.546494  35.765381  28.612303  31.473534  62546380.0   \n1 1999-11-19      A  30.713518  30.758226  28.478184  28.880545  15234146.0   \n2 1999-11-22      A  29.551144  31.473534  28.657009  31.473534   6577870.0   \n3 1999-11-23      A  30.400572  31.205294  28.612303  28.612303   5975611.0   \n4 1999-11-24      A  28.701717  29.998213  28.612303  29.372318   4843231.0   \n\n   dividend  ex_div_flag div_yield_proxy  \n0       0.0            0             0.0  \n1       0.0            0             0.0  \n2       0.0            0             0.0  \n3       0.0            0             0.0  \n4       0.0            0             0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>ticker</th>\n      <th>open_adj</th>\n      <th>high_adj</th>\n      <th>low_adj</th>\n      <th>close_adj</th>\n      <th>volume_adj</th>\n      <th>dividend</th>\n      <th>ex_div_flag</th>\n      <th>div_yield_proxy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1999-11-18</td>\n      <td>A</td>\n      <td>32.546494</td>\n      <td>35.765381</td>\n      <td>28.612303</td>\n      <td>31.473534</td>\n      <td>62546380.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1999-11-19</td>\n      <td>A</td>\n      <td>30.713518</td>\n      <td>30.758226</td>\n      <td>28.478184</td>\n      <td>28.880545</td>\n      <td>15234146.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1999-11-22</td>\n      <td>A</td>\n      <td>29.551144</td>\n      <td>31.473534</td>\n      <td>28.657009</td>\n      <td>31.473534</td>\n      <td>6577870.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1999-11-23</td>\n      <td>A</td>\n      <td>30.400572</td>\n      <td>31.205294</td>\n      <td>28.612303</td>\n      <td>28.612303</td>\n      <td>5975611.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1999-11-24</td>\n      <td>A</td>\n      <td>28.701717</td>\n      <td>29.998213</td>\n      <td>28.612303</td>\n      <td>29.372318</td>\n      <td>4843231.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# 1) Do we have any non-zero dividends in the raw dividends file?\nprint(\"non-zero dividends rows in df_div:\", (df_div[\"dividend\"] > 0).sum())\nprint(df_div.head())\nprint(df_div.describe(include='all'))\n\n# 2) After merge: any non-zero dividends for the whole df?\nprint(\"non-zero merged dividends:\", (df[\"dividend\"] > 0).sum())\n\n# 3) Check the current ticker in your screenshot (looks like 'A')\nsym = df.loc[0, \"ticker\"]  # or set sym = \"A\"\nprint(\"Ticker under view:\", sym)\n\nprint(\"dividends>0 rows for that ticker in df_div:\")\nprint(df_div[(df_div[\"ticker\"]==sym) & (df_div[\"dividend\"]>0)].sort_values(\"date\").head(10))\n\n# 4) Are your dates aligned (both datetime)?\nprint(df[\"date\"].dtype, df_div[\"date\"].dtype)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T02:43:38.558089Z","iopub.execute_input":"2025-11-09T02:43:38.558704Z","iopub.status.idle":"2025-11-09T02:43:38.649252Z","shell.execute_reply.started":"2025-11-09T02:43:38.558671Z","shell.execute_reply":"2025-11-09T02:43:38.648215Z"}},"outputs":[{"name":"stdout","text":"non-zero dividends rows in df_div: 164540\n  ticker       date  dividend\n0   AACG 2011-06-28     0.430\n1   AACG 2012-08-16     0.174\n2   AACG 2014-06-26     0.410\n3   AACG 2017-06-08     0.410\n4   AACG 2018-08-27     6.000\n        ticker                           date      dividend\ncount   164980                         164980  1.649800e+05\nunique    2485                            NaN           NaN\ntop        PBT                            NaN           NaN\nfreq       450                            NaN           NaN\nmean       NaN  2009-11-08 17:00:10.037580288  2.419493e+09\nmin        NaN            1977-12-30 00:00:00  0.000000e+00\n25%        NaN            2002-04-26 00:00:00  6.564800e-02\n50%        NaN            2012-02-15 00:00:00  1.500000e-01\n75%        NaN            2018-09-27 00:00:00  3.200000e-01\nmax        NaN            2024-07-03 00:00:00  2.268000e+14\nstd        NaN                            NaN  6.271019e+11\nnon-zero merged dividends: 164502\nTicker under view: A\ndividends>0 rows for that ticker in df_div:\n     ticker       date  dividend\n1690      A 2006-11-01  1.471388\n1691      A 2012-03-30  0.071531\n1692      A 2012-06-29  0.071531\n1693      A 2012-09-28  0.071531\n1694      A 2012-12-27  0.071531\n1695      A 2013-03-28  0.085837\n1696      A 2013-06-28  0.085837\n1697      A 2013-09-27  0.085837\n1698      A 2013-12-27  0.094421\n1699      A 2014-04-09  0.094421\ndatetime64[ns] datetime64[ns]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Features + “shorting” labelling ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndf = df.sort_values([\"ticker\",\"date\"]).reset_index(drop=True)\n# 1-day return from adjusted close\ndf[\"ret1\"] = df.groupby(\"ticker\")[\"close_adj\"].pct_change()\n\n# Rolling vol 20, MAs and ratio\ndf[\"rvol20\"] = df.groupby(\"ticker\")[\"ret1\"].rolling(20).std().reset_index(level=0, drop=True)\ndf[\"ma10\"]   = df.groupby(\"ticker\")[\"close_adj\"].rolling(10).mean().reset_index(level=0, drop=True)\ndf[\"ma20\"]   = df.groupby(\"ticker\")[\"close_adj\"].rolling(20).mean().reset_index(level=0, drop=True)\ndf[\"ma_ratio\"] = df[\"ma10\"]/df[\"ma20\"]\n\n# RSI(14)\ndelta = df.groupby(\"ticker\")[\"close_adj\"].diff()\ngain  = delta.clip(lower=0).groupby(df[\"ticker\"]).rolling(14).mean().reset_index(level=0, drop=True)\nloss  = (-delta.clip(upper=0)).groupby(df[\"ticker\"]).rolling(14).mean().reset_index(level=0, drop=True)\nrs = gain / (loss.replace(0, np.nan))\ndf[\"rsi14\"] = 100 - (100/(1+rs))\n\n# Shorting label: drop ≥3% within next 5 trading days\nN, thr = 5, -0.03\nfuture_close = df.groupby(\"ticker\")[\"close_adj\"].shift(-N)\ndf[\"future_retN\"] = (future_close - df[\"close_adj\"]) / df[\"close_adj\"]\ndf[\"y_short\"] = (df[\"future_retN\"] <= thr).astype(int)\n\n# Tidy\nfeatures = [\"ret1\",\"rvol20\",\"ma_ratio\",\"rsi14\",\"volume_adj\",\"ex_div_flag\",\"div_yield_proxy\"]\nmodel_df = df[[\"date\",\"ticker\",\"close_adj\",\"future_retN\",\"y_short\"] + features].dropna().reset_index(drop=True)\nmodel_df.head(), model_df.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T02:43:44.030294Z","iopub.execute_input":"2025-11-09T02:43:44.030562Z","iopub.status.idle":"2025-11-09T02:44:58.231366Z","shell.execute_reply.started":"2025-11-09T02:43:44.030541Z","shell.execute_reply":"2025-11-09T02:44:58.230581Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(        date ticker  close_adj  future_retN  y_short      ret1    rvol20  \\\n 0 1999-12-17      A  32.859444     0.149660        0 -0.027778  0.051155   \n 1 1999-12-20      A  33.530045     0.312000        0  0.020408  0.047086   \n 2 1999-12-21      A  33.351215     0.544236        0 -0.005333  0.043080   \n 3 1999-12-22      A  34.021816     0.666229        0  0.020107  0.036956   \n 4 1999-12-23      A  35.586552     0.554020        0  0.045992  0.037681   \n \n    ma_ratio      rsi14  volume_adj  ex_div_flag div_yield_proxy  \n 0  1.031657  61.010842   3708055.0            0             0.0  \n 1  1.026589  62.886612   1196828.0            0             0.0  \n 2  1.026652  60.424029   2259448.0            0             0.0  \n 3  1.023262  59.856632   1905754.0            0             0.0  \n 4  1.022133  63.636367   2159491.0            0             0.0  ,\n (22629771, 12))"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Robust feature clipping (winsorize)","metadata":{}},{"cell_type":"code","source":"caps = {\n    \"ret1\":    (-0.5, 0.5),  # -50%..+50% daily\n    \"rvol20\":  (0,    model_df[\"rvol20\"].quantile(0.99)),\n    \"ma_ratio\":(0.5,  1.5),\n    \"rsi14\":   (0,    100)\n}\nfor col, (lo, hi) in caps.items():\n    if col in model_df.columns:\n        model_df[col] = model_df[col].clip(lo, hi)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T02:45:08.532790Z","iopub.execute_input":"2025-11-09T02:45:08.533065Z","iopub.status.idle":"2025-11-09T02:45:09.988704Z","shell.execute_reply.started":"2025-11-09T02:45:08.533044Z","shell.execute_reply":"2025-11-09T02:45:09.987899Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Train/test split + Pipeline training + baseline model + calibration","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, average_precision_score, f1_score\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Date split\nsplit_date = pd.Timestamp(\"2023-01-01\")\ntrain = model_df[model_df[\"date\"] < split_date].copy()\ntest  = model_df[model_df[\"date\"] >= split_date].copy()\n\nXtr, ytr = train[features], train[\"y_short\"]\nXte, yte = test[features],  test[\"y_short\"]\n\n# Pipeline\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"clf\", LogisticRegression(\n        max_iter=1000,\n        class_weight=\"balanced\",\n        C=3.0,\n        solver=\"lbfgs\",\n        n_jobs=-1\n    ))\n])\npipe.fit(Xtr, ytr)\n\n# Optional calibration\nUSE_CALIBRATION = True\nif USE_CALIBRATION:\n    cal = CalibratedClassifierCV(pipe, method=\"isotonic\", cv=3)\n    cal.fit(Xtr, ytr)\n    INFER_MODEL = cal\nelse:\n    INFER_MODEL = pipe\n\n# Predictions & metrics\np = INFER_MODEL.predict_proba(Xte)[:,1]\nthr_space = np.linspace(0.05, 0.95, 19)\nf1s = [f1_score(yte, (p>t).astype(int)) for t in thr_space]\nbest_thr = float(thr_space[int(np.argmax(f1s))])\nbest_f1  = float(max(f1s))\n\nmetrics = {\n    \"auroc\": float(roc_auc_score(yte, p)),\n    \"pr_auc\": float(average_precision_score(yte, p)),\n    \"f1_pos@best_thr\": best_f1,\n    \"best_thr\": best_thr\n}\n\n# Precision@10 (STEP 3)\nidx_top = np.argsort(-p)[:10]\nprecision_at_10 = float((yte.values[idx_top] == 1).mean())\nmetrics[\"precision@10\"] = precision_at_10\n\nprint(\"Metrics:\", metrics)\nprint(\"Prob range:\", p.min(), p.max())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T02:45:16.314403Z","iopub.execute_input":"2025-11-09T02:45:16.314689Z","iopub.status.idle":"2025-11-09T02:49:10.400837Z","shell.execute_reply.started":"2025-11-09T02:45:16.314668Z","shell.execute_reply":"2025-11-09T02:49:10.400028Z"}},"outputs":[{"name":"stdout","text":"Metrics: {'auroc': 0.6518385096349658, 'pr_auc': 0.40675996869868347, 'f1_pos@best_thr': 0.4815502546074781, 'best_thr': 0.25, 'precision@10': 0.6}\nProb range: 0.024205005804655084 0.669852648347272\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Qdrant (embedded) + semantic notes","metadata":{}},{"cell_type":"code","source":"!pip -q install qdrant-client fastembed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:12:06.316657Z","iopub.execute_input":"2025-11-09T03:12:06.316987Z","iopub.status.idle":"2025-11-09T03:12:14.261246Z","shell.execute_reply.started":"2025-11-09T03:12:06.316967Z","shell.execute_reply":"2025-11-09T03:12:14.260431Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from qdrant_client import QdrantClient\nfrom qdrant_client.http.models import Distance, VectorParams, PointStruct\n\n# Try FastEmbed first; fall back to sentence-transformers\ntry:\n    from fastembed import TextEmbedding\n    EMB_BACKEND = \"fastembed\"\nexcept Exception:\n    EMB_BACKEND = \"sbert\"\n    try:\n        from sentence_transformers import SentenceTransformer\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\"sentence-transformers not installed. Run:\\n!pip -q install sentence-transformers\")\n\n# Sample notes\nctx = test.sort_values(\"date\").tail(10_000).copy()\nctx[\"note\"] = [\n    f\"ticker:{t}; date:{d:%Y-%m-%d}; ret1:{r:.4f}; rvol20:{v:.4f}; ma_ratio:{m:.3f}; rsi14:{s:.2f}\"\n    for t,d,r,v,m,s in zip(ctx[\"ticker\"], ctx[\"date\"], ctx[\"ret1\"], ctx[\"rvol20\"], ctx[\"ma_ratio\"], ctx[\"rsi14\"])\n]\n\n# Embed\nif EMB_BACKEND == \"fastembed\":\n    embedder = TextEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n                             cache_dir=\"/kaggle/working/.emb\")\n    vectors = list(embedder.embed(ctx[\"note\"].tolist()))  # generator -> list\n    dim = len(vectors[0])\nelse:\n    sbert = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\",\n                                cache_folder=\"/kaggle/working/.emb\")\n    vectors = sbert.encode(ctx[\"note\"].tolist(),\n                           normalize_embeddings=True,\n                           convert_to_numpy=True,\n                           show_progress_bar=False)\n    dim = vectors.shape[1]  # 384\n\n# Qdrant (in-memory)\nqdr = QdrantClient(path=\":memory:\")\nCOL = \"shortsight_notes\"\nqdr.recreate_collection(COL, vectors_config=VectorParams(size=dim, distance=Distance.COSINE))\n\npoints = [PointStruct(\n    id=i,\n    vector=vectors[i],\n    payload={\"ticker\": ctx[\"ticker\"].iloc[i],\n             \"date\": str(ctx[\"date\"].iloc[i].date()),\n             \"note\": ctx[\"note\"].iloc[i]}\n) for i in range(len(ctx))]\nqdr.upsert(collection_name=COL, points=points)\n\n# Helper to embed a query (handles both backends)\ndef _embed_query(text: str):\n    if 'embedder' in globals():            # FastEmbed\n        return next(embedder.embed([text]))\n    elif 'sbert' in globals():             # Sentence-Transformers\n        return sbert.encode([text], normalize_embeddings=True, convert_to_numpy=True)[0]\n    else:\n        raise RuntimeError(\"No embedding backend available.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:12:59.193431Z","iopub.execute_input":"2025-11-09T03:12:59.193762Z","iopub.status.idle":"2025-11-09T03:14:27.973075Z","shell.execute_reply.started":"2025-11-09T03:12:59.193734Z","shell.execute_reply":"2025-11-09T03:14:27.972281Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b23d1c2756c49329d039c99f54d1f4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c1fb8184ddd4ddcbbf945e6df127308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a741257250314212bf78b3cdefee6279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4c745b9da8e466193dc3a5b334b3a8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed6d4615f4864971a6697fde1764692b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb07f994683d4f5bb85fbec6d0c6b89c"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/2491246734.py:40: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n  qdr.recreate_collection(COL, vectors_config=VectorParams(size=dim, distance=Distance.COSINE))\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Langraph (3 agents)","metadata":{}},{"cell_type":"code","source":"!pip install -q langgraph\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:16:03.889685Z","iopub.execute_input":"2025-11-09T03:16:03.889979Z","iopub.status.idle":"2025-11-09T03:16:09.498007Z","shell.execute_reply.started":"2025-11-09T03:16:03.889958Z","shell.execute_reply":"2025-11-09T03:16:09.497279Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.2/471.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.4 which is incompatible.\nlangchain-text-splitters 0.3.9 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from langgraph.graph import StateGraph\nfrom typing import TypedDict, List, Dict, Any\n\n# sanity asserts\nassert 'INFER_MODEL' in globals()\nassert 'features' in globals()\nassert 'test' in globals()\nassert 'metrics' in globals()\nassert 'qdr' in globals() and 'COL' in globals()\n\nclass State(TypedDict):\n    query: str\n    retrieved: List[Dict[str,Any]]\n    model_metrics: Dict[str,float]\n    today_scores: pd.DataFrame\n    picks: pd.DataFrame\n    narrative: str\n\nstate: State = {\n    \"query\": \"\",\n    \"retrieved\": [],\n    \"model_metrics\": metrics,\n    \"today_scores\": pd.DataFrame(),\n    \"picks\": pd.DataFrame(),\n    \"narrative\": \"\"\n}\n\ndef analyst_node(s: State) -> State:\n    if test.empty:\n        s[\"query\"] = \"no test data available\"\n        s[\"retrieved\"] = []\n        return s\n    today = test[\"date\"].max()\n    today_df = test[test[\"date\"] == today].copy()\n    if today_df.empty:\n        s[\"query\"] = \"no rows for latest date\"\n        s[\"retrieved\"] = []\n        return s\n    hottest = today_df.nlargest(min(5, len(today_df)), \"rvol20\")[\"ticker\"].tolist()\n    s[\"query\"] = f\"focus on tickers: {', '.join(hottest)} with elevated rvol20 on {today.date()}\"\n    q_vec = _embed_query(s[\"query\"])\n    hits = qdr.search(collection_name=COL, query_vector=q_vec, limit=10)\n    s[\"retrieved\"] = [h.payload for h in hits]\n    return s\n\ndef model_node(s: State) -> State:\n    if test.empty:\n        s[\"today_scores\"] = pd.DataFrame()\n        return s\n\n    today = test[\"date\"].max()\n    day = test[test[\"date\"] == today].copy()\n    if day.empty:\n        s[\"today_scores\"] = pd.DataFrame()\n        return s\n\n    # probabilities using SAME trained model\n    day[\"prob\"] = INFER_MODEL.predict_proba(day[features])[:, 1]\n\n    # ---- STEP 1: Clean picks filters (liquidity/price; display clips only) ----\n    MIN_PRICE = 3.0\n    MIN_VOL   = 50_000\n    if set([\"close_adj\",\"volume_adj\"]).issubset(day.columns):\n        clean = day[(day[\"close_adj\"] >= MIN_PRICE) & (day[\"volume_adj\"] >= MIN_VOL)].copy()\n    else:\n        clean = day.copy()\n\n    if \"ret1\" in clean:\n        clean[\"ret1\"] = clean[\"ret1\"].clip(-0.5, 0.5)\n    if \"rvol20\" in clean and clean[\"rvol20\"].notna().any():\n        q99 = np.nanquantile(clean[\"rvol20\"], 0.99)\n        clean[\"rvol20\"] = clean[\"rvol20\"].clip(0, q99)\n\n    s[\"today_scores\"] = clean[[\"date\",\"ticker\",\"prob\",\"rsi14\",\"ma_ratio\",\"rvol20\",\"ret1\"]].sort_values(\"prob\", ascending=False)\n    s[\"model_metrics\"] = metrics\n    return s\n\ndef risk_node(s: State) -> State:\n    K = 10\n    if s[\"today_scores\"].empty:\n        s[\"picks\"] = pd.DataFrame()\n        s[\"narrative\"] = \"No scores available.\"\n        return s\n\n    picks = s[\"today_scores\"].head(K).copy()\n    s[\"picks\"] = picks\n\n    bullets = []\n    med_vol = np.nanmedian(test[\"rvol20\"]) if \"rvol20\" in test.columns else np.nan\n    for _, r in picks.iterrows():\n        why = []\n        if \"rsi14\" in r and r[\"rsi14\"] > 70:   why.append(\"overbought RSI\")\n        if \"ma_ratio\" in r and r[\"ma_ratio\"] < 1.0: why.append(\"MA10 < MA20\")\n        if not np.isnan(med_vol) and r.get(\"rvol20\", np.nan) > med_vol: why.append(\"elevated vol\")\n        if not why: why = [\"pattern-based downside risk\"]\n        bullets.append(f\"- {r['ticker']}: \" + \", \".join(why))\n    s[\"narrative\"] = \"Top short candidates (rule-based):\\n\" + \"\\n\".join(bullets)\n    return s\n\ng = StateGraph(State)\ng.add_node(\"analyst\", analyst_node)\ng.add_node(\"model\",   model_node)\ng.add_node(\"risk\",    risk_node)\ng.add_edge(\"analyst\",\"model\")\ng.add_edge(\"model\",\"risk\")\ng.set_entry_point(\"analyst\")\ng.set_finish_point(\"risk\")\napp = g.compile()\n\nresult = app.invoke(state)\n\nprint(\"Retrieved (Qdrant) hits:\", len(result[\"retrieved\"]))\nprint(\"Today scores shape:\", result[\"today_scores\"].shape)\nprint(\"Top picks shape:\", result[\"picks\"].shape)\nprint(\"Metrics:\", result[\"model_metrics\"])\ndisplay(result[\"picks\"].head(10))\nprint(result[\"narrative\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:16:15.187956Z","iopub.execute_input":"2025-11-09T03:16:15.188637Z","iopub.status.idle":"2025-11-09T03:16:16.140161Z","shell.execute_reply.started":"2025-11-09T03:16:15.188590Z","shell.execute_reply":"2025-11-09T03:16:16.139392Z"}},"outputs":[{"name":"stdout","text":"Retrieved (Qdrant) hits: 10\nToday scores shape: (3231, 7)\nTop picks shape: (10, 7)\nMetrics: {'auroc': 0.6518385096349658, 'pr_auc': 0.40675996869868347, 'f1_pos@best_thr': 0.4815502546074781, 'best_thr': 0.25, 'precision@10': 0.6}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/1219946857.py:42: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n  hits = qdr.search(collection_name=COL, query_vector=q_vec, limit=10)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"               date ticker      prob      rsi14  ma_ratio    rvol20      ret1\n4454476  2024-06-26   CMAX  0.598266  56.118143  0.994687  0.102203  0.500000\n15038770 2024-06-26   OPTX  0.598266  67.441860  0.965395  0.102203  0.500000\n21304849 2024-06-26   VLCN  0.563530  26.933514  0.510482  0.102203  0.187050\n10137029 2024-06-26    ICU  0.551773  44.925256  0.841832  0.102203  0.188679\n2971016  2024-06-26   BNED  0.525310  11.084482  0.500000  0.102203  0.062027\n12959760 2024-06-26   MLGO  0.516021  75.764075  1.256643  0.102203  0.128659\n12674579 2024-06-26   MEDS  0.500164  74.211045  1.130902  0.102203  0.351197\n697611   2024-06-26   AISP  0.418930  50.081037  1.050101  0.102203 -0.118863\n22598677 2024-06-26   ZNTL  0.418930  11.431514  0.724252  0.102203 -0.009524\n3104228  2024-06-26   BRFH  0.418930  80.626781  1.271920  0.102203  0.134969","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>ticker</th>\n      <th>prob</th>\n      <th>rsi14</th>\n      <th>ma_ratio</th>\n      <th>rvol20</th>\n      <th>ret1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4454476</th>\n      <td>2024-06-26</td>\n      <td>CMAX</td>\n      <td>0.598266</td>\n      <td>56.118143</td>\n      <td>0.994687</td>\n      <td>0.102203</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>15038770</th>\n      <td>2024-06-26</td>\n      <td>OPTX</td>\n      <td>0.598266</td>\n      <td>67.441860</td>\n      <td>0.965395</td>\n      <td>0.102203</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>21304849</th>\n      <td>2024-06-26</td>\n      <td>VLCN</td>\n      <td>0.563530</td>\n      <td>26.933514</td>\n      <td>0.510482</td>\n      <td>0.102203</td>\n      <td>0.187050</td>\n    </tr>\n    <tr>\n      <th>10137029</th>\n      <td>2024-06-26</td>\n      <td>ICU</td>\n      <td>0.551773</td>\n      <td>44.925256</td>\n      <td>0.841832</td>\n      <td>0.102203</td>\n      <td>0.188679</td>\n    </tr>\n    <tr>\n      <th>2971016</th>\n      <td>2024-06-26</td>\n      <td>BNED</td>\n      <td>0.525310</td>\n      <td>11.084482</td>\n      <td>0.500000</td>\n      <td>0.102203</td>\n      <td>0.062027</td>\n    </tr>\n    <tr>\n      <th>12959760</th>\n      <td>2024-06-26</td>\n      <td>MLGO</td>\n      <td>0.516021</td>\n      <td>75.764075</td>\n      <td>1.256643</td>\n      <td>0.102203</td>\n      <td>0.128659</td>\n    </tr>\n    <tr>\n      <th>12674579</th>\n      <td>2024-06-26</td>\n      <td>MEDS</td>\n      <td>0.500164</td>\n      <td>74.211045</td>\n      <td>1.130902</td>\n      <td>0.102203</td>\n      <td>0.351197</td>\n    </tr>\n    <tr>\n      <th>697611</th>\n      <td>2024-06-26</td>\n      <td>AISP</td>\n      <td>0.418930</td>\n      <td>50.081037</td>\n      <td>1.050101</td>\n      <td>0.102203</td>\n      <td>-0.118863</td>\n    </tr>\n    <tr>\n      <th>22598677</th>\n      <td>2024-06-26</td>\n      <td>ZNTL</td>\n      <td>0.418930</td>\n      <td>11.431514</td>\n      <td>0.724252</td>\n      <td>0.102203</td>\n      <td>-0.009524</td>\n    </tr>\n    <tr>\n      <th>3104228</th>\n      <td>2024-06-26</td>\n      <td>BRFH</td>\n      <td>0.418930</td>\n      <td>80.626781</td>\n      <td>1.271920</td>\n      <td>0.102203</td>\n      <td>0.134969</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Top short candidates (rule-based):\n- CMAX: MA10 < MA20, elevated vol\n- OPTX: MA10 < MA20, elevated vol\n- VLCN: MA10 < MA20, elevated vol\n- ICU: MA10 < MA20, elevated vol\n- BNED: MA10 < MA20, elevated vol\n- MLGO: overbought RSI, elevated vol\n- MEDS: overbought RSI, elevated vol\n- AISP: elevated vol\n- ZNTL: MA10 < MA20, elevated vol\n- BRFH: overbought RSI, elevated vol\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"****Sanity check****","metadata":{}},{"cell_type":"code","source":"result = app.invoke(state)\nresult[\"picks\"].head(10), result[\"model_metrics\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:16:33.515398Z","iopub.execute_input":"2025-11-09T03:16:33.515717Z","iopub.status.idle":"2025-11-09T03:16:33.627120Z","shell.execute_reply.started":"2025-11-09T03:16:33.515693Z","shell.execute_reply":"2025-11-09T03:16:33.626217Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_48/1219946857.py:42: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n  hits = qdr.search(collection_name=COL, query_vector=q_vec, limit=10)\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(               date ticker      prob      rsi14  ma_ratio    rvol20      ret1\n 4454476  2024-06-26   CMAX  0.598266  56.118143  0.994687  0.102203  0.500000\n 15038770 2024-06-26   OPTX  0.598266  67.441860  0.965395  0.102203  0.500000\n 21304849 2024-06-26   VLCN  0.563530  26.933514  0.510482  0.102203  0.187050\n 10137029 2024-06-26    ICU  0.551773  44.925256  0.841832  0.102203  0.188679\n 2971016  2024-06-26   BNED  0.525310  11.084482  0.500000  0.102203  0.062027\n 12959760 2024-06-26   MLGO  0.516021  75.764075  1.256643  0.102203  0.128659\n 12674579 2024-06-26   MEDS  0.500164  74.211045  1.130902  0.102203  0.351197\n 697611   2024-06-26   AISP  0.418930  50.081037  1.050101  0.102203 -0.118863\n 22598677 2024-06-26   ZNTL  0.418930  11.431514  0.724252  0.102203 -0.009524\n 3104228  2024-06-26   BRFH  0.418930  80.626781  1.271920  0.102203  0.134969,\n {'auroc': 0.6518385096349658,\n  'pr_auc': 0.40675996869868347,\n  'f1_pos@best_thr': 0.4815502546074781,\n  'best_thr': 0.25,\n  'precision@10': 0.6})"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"# Save artifacts (metrics, picks, narrative)","metadata":{}},{"cell_type":"code","source":"ART = Path(\"/kaggle/working/artifacts\")\nART.mkdir(parents=True, exist_ok=True)\n\nresult[\"today_scores\"].to_csv(ART/\"today_scores.csv\", index=False)\nresult[\"picks\"].to_csv(ART/\"picks.csv\", index=False)\npd.Series(metrics).to_csv(ART/\"metrics.csv\")\nwith open(ART/\"narrative.txt\",\"w\") as f:\n    f.write(result[\"narrative\"])\n\nprint(\"Artifacts saved:\", [p.name for p in ART.iterdir()])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:16:36.286068Z","iopub.execute_input":"2025-11-09T03:16:36.286841Z","iopub.status.idle":"2025-11-09T03:16:36.341699Z","shell.execute_reply.started":"2025-11-09T03:16:36.286795Z","shell.execute_reply":"2025-11-09T03:16:36.340905Z"}},"outputs":[{"name":"stdout","text":"Artifacts saved: ['picks.csv', 'narrative.txt', 'today_scores.csv', 'metrics.csv']\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Optional backtest (respects clean filters)","metadata":{}},{"cell_type":"code","source":"K, N = 5, 5\nscores = test.copy()\nscores[\"prob\"] = INFER_MODEL.predict_proba(scores[features])[:,1]\n\n# same filters as model_node\nscores = scores[(scores[\"close_adj\"] >= 3.0) & (scores[\"volume_adj\"] >= 50_000)]\n\ndates = sorted(scores[\"date\"].unique())\ndaily_ret = []\nfor d in dates[:-N]:\n    day = scores[scores[\"date\"]==d].sort_values(\"prob\", ascending=False).head(K)\n    pnl = (-day[\"future_retN\"]).clip(-0.05, 0.05).mean() if len(day) else 0.0\n    daily_ret.append(pnl)\n\nequity = (1+pd.Series(daily_ret)).cumprod()\nequity_df = pd.DataFrame({\"step\": range(len(equity)), \"equity\": equity.values})\nequity_df.to_csv(ART/\"equity_curve.csv\", index=False)\nprint(\"Backtest saved:\", ART/\"equity_curve.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:39:18.123034Z","iopub.execute_input":"2025-11-09T03:39:18.123920Z","iopub.status.idle":"2025-11-09T03:39:27.020584Z","shell.execute_reply.started":"2025-11-09T03:39:18.123894Z","shell.execute_reply":"2025-11-09T03:39:27.019717Z"}},"outputs":[{"name":"stdout","text":"Backtest saved: /kaggle/working/artifacts/equity_curve.csv\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import joblib, os\nfrom pathlib import Path\nART = Path(\"/kaggle/working/artifacts\"); ART.mkdir(parents=True, exist_ok=True)\njoblib.dump(INFER_MODEL, ART/\"model_pipeline.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:40:44.811151Z","iopub.execute_input":"2025-11-09T03:40:44.811776Z","iopub.status.idle":"2025-11-09T03:40:44.822637Z","shell.execute_reply.started":"2025-11-09T03:40:44.811752Z","shell.execute_reply":"2025-11-09T03:40:44.821985Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/artifacts/model_pipeline.pkl']"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:40:57.811014Z","iopub.execute_input":"2025-11-09T03:40:57.811779Z","iopub.status.idle":"2025-11-09T03:40:57.815277Z","shell.execute_reply.started":"2025-11-09T03:40:57.811753Z","shell.execute_reply":"2025-11-09T03:40:57.814469Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# INTERACTIVE KAGGLE DASHBOARD\n\nfrom IPython.display import display, HTML, clear_output\nimport ipywidgets as widgets\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Beautiful header\ndisplay(HTML(\"\"\"\n<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n            padding: 40px; border-radius: 15px; margin-bottom: 30px; text-align: center;'>\n    <h1 style='color: white; font-size: 48px; margin: 0;'>🧠 AI Multi-Agent Stock Shorting</h1>\n    <p style='color: white; font-size: 24px; margin-top: 15px; opacity: 0.95;'>\n        Short Candidates Detection System\n    </p>\n    <p style='color: white; font-size: 16px; margin-top: 10px; opacity: 0.85;'>\n        LangGraph • Qdrant • Logistic Regression • Vector Search\n    </p>\n</div>\n\"\"\"))\n\n# Metrics display\ncol_html = f\"\"\"\n<div style='display: grid; grid-template-columns: repeat(4, 1fr); gap: 20px; margin-bottom: 30px;'>\n    <div style='background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); text-align: center;'>\n        <div style='font-size: 14px; color: #666; margin-bottom: 8px;'>AUROC</div>\n        <div style='font-size: 32px; font-weight: bold; color: #667eea;'>{metrics.get('auroc', 0):.3f}</div>\n    </div>\n    <div style='background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); text-align: center;'>\n        <div style='font-size: 14px; color: #666; margin-bottom: 8px;'>PR-AUC</div>\n        <div style='font-size: 32px; font-weight: bold; color: #764ba2;'>{metrics.get('pr_auc', 0):.3f}</div>\n    </div>\n    <div style='background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); text-align: center;'>\n        <div style='font-size: 14px; color: #666; margin-bottom: 8px;'>Best Threshold</div>\n        <div style='font-size: 32px; font-weight: bold; color: #e74c3c;'>{metrics.get('best_thr', 0):.2f}</div>\n    </div>\n    <div style='background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); text-align: center;'>\n        <div style='font-size: 14px; color: #666; margin-bottom: 8px;'>Precision@10</div>\n        <div style='font-size: 32px; font-weight: bold; color: #27ae60;'>{metrics.get('precision@10', 0):.2f}</div>\n    </div>\n</div>\n\"\"\"\ndisplay(HTML(col_html))\n\nprint(\"📊 MODEL PERFORMANCE METRICS LOADED\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:41:02.374795Z","iopub.execute_input":"2025-11-09T03:41:02.375409Z","iopub.status.idle":"2025-11-09T03:41:03.150983Z","shell.execute_reply.started":"2025-11-09T03:41:02.375386Z","shell.execute_reply":"2025-11-09T03:41:03.150208Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n            padding: 40px; border-radius: 15px; margin-bottom: 30px; text-align: center;'>\n    <h1 style='color: white; font-size: 48px; margin: 0;'>🧠 AI Multi-Agent Stock Shorting</h1>\n    <p style='color: white; font-size: 24px; margin-top: 15px; opacity: 0.95;'>\n        Short Candidates Detection System\n    </p>\n    <p style='color: white; font-size: 16px; margin-top: 10px; opacity: 0.85;'>\n        LangGraph • Qdrant • Logistic Regression • Vector Search\n    </p>\n</div>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<div style='display: grid; grid-template-columns: repeat(4, 1fr); gap: 20px; margin-bottom: 30px;'>\n    <div style='background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); text-align: center;'>\n        <div style='font-size: 14px; color: #666; margin-bottom: 8px;'>AUROC</div>\n        <div style='font-size: 32px; font-weight: bold; color: #667eea;'>0.652</div>\n    </div>\n    <div style='background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); text-align: center;'>\n        <div style='font-size: 14px; color: #666; margin-bottom: 8px;'>PR-AUC</div>\n        <div style='font-size: 32px; font-weight: bold; color: #764ba2;'>0.407</div>\n    </div>\n    <div style='background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); text-align: center;'>\n        <div style='font-size: 14px; color: #666; margin-bottom: 8px;'>Best Threshold</div>\n        <div style='font-size: 32px; font-weight: bold; color: #e74c3c;'>0.25</div>\n    </div>\n    <div style='background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); text-align: center;'>\n        <div style='font-size: 14px; color: #666; margin-bottom: 8px;'>Precision@10</div>\n        <div style='font-size: 32px; font-weight: bold; color: #27ae60;'>0.60</div>\n    </div>\n</div>\n"},"metadata":{}},{"name":"stdout","text":"📊 MODEL PERFORMANCE METRICS LOADED\n======================================================================\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import plotly.io as pio\npio.renderers.default = \"iframe_connected\"\nimport plotly.graph_objects as go\n\npicks_df = result[\"picks\"].head(10).copy()  # just top 10\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=picks_df[\"ticker\"],\n    y=picks_df[\"prob\"],\n    marker=dict(\n        color=picks_df[\"prob\"],\n        colorscale=\"Reds\",\n        showscale=True,\n        colorbar=dict(title=\"Short Probability\")\n    ),\n    text=[f\"{p:.2%}\" for p in picks_df[\"prob\"]],\n    textposition=\"outside\",\n    hovertemplate=\"<b>%{x}</b><br>Probability: %{y:.2%}<extra></extra>\"\n))\n\nfig.update_layout(\n    title=\"🎯 Top 10 Short Candidates by ML Probability\",\n    xaxis_title=\"Stock Ticker\",\n    yaxis_title=\"Short Probability\",\n    template=\"plotly_white\",\n    height=500,\n    font=dict(size=14),\n    title_font=dict(size=20, color=\"#2c3e50\")\n)\n\nfig.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:49:03.769726Z","iopub.execute_input":"2025-11-09T03:49:03.770016Z","iopub.status.idle":"2025-11-09T03:49:03.805634Z","shell.execute_reply.started":"2025-11-09T03:49:03.769995Z","shell.execute_reply":"2025-11-09T03:49:03.804926Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"520\"\n    src=\"iframe_figures/figure_41.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"print(\"\\n📋 DETAILED SHORT CANDIDATES:\")\nprint(\"=\"*90)\nfor idx, (i, row) in enumerate(picks_df.iterrows(), 1):\n    print(f\"{idx:2}. {row['ticker']:6} | Prob: {row['prob']:.2%} | RSI: {row['rsi14']:.1f} | MA Ratio: {row['ma_ratio']:.3f} | Vol: {row['rvol20']:.4f}\")\nprint(\"=\"*90)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:49:11.621539Z","iopub.execute_input":"2025-11-09T03:49:11.622405Z","iopub.status.idle":"2025-11-09T03:49:11.628461Z","shell.execute_reply.started":"2025-11-09T03:49:11.622377Z","shell.execute_reply":"2025-11-09T03:49:11.627661Z"}},"outputs":[{"name":"stdout","text":"\n📋 DETAILED SHORT CANDIDATES:\n==========================================================================================\n 1. CMAX   | Prob: 59.83% | RSI: 56.1 | MA Ratio: 0.995 | Vol: 0.1022\n 2. OPTX   | Prob: 59.83% | RSI: 67.4 | MA Ratio: 0.965 | Vol: 0.1022\n 3. VLCN   | Prob: 56.35% | RSI: 26.9 | MA Ratio: 0.510 | Vol: 0.1022\n 4. ICU    | Prob: 55.18% | RSI: 44.9 | MA Ratio: 0.842 | Vol: 0.1022\n 5. BNED   | Prob: 52.53% | RSI: 11.1 | MA Ratio: 0.500 | Vol: 0.1022\n 6. MLGO   | Prob: 51.60% | RSI: 75.8 | MA Ratio: 1.257 | Vol: 0.1022\n 7. MEDS   | Prob: 50.02% | RSI: 74.2 | MA Ratio: 1.131 | Vol: 0.1022\n 8. AISP   | Prob: 41.89% | RSI: 50.1 | MA Ratio: 1.050 | Vol: 0.1022\n 9. ZNTL   | Prob: 41.89% | RSI: 11.4 | MA Ratio: 0.724 | Vol: 0.1022\n10. BRFH   | Prob: 41.89% | RSI: 80.6 | MA Ratio: 1.272 | Vol: 0.1022\n==========================================================================================\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# COMPREHENSIVE VISUALIZATION DASHBOARD\n\n# Create subplots\nfig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=(\n        'Risk Score Distribution',\n        'RSI vs Probability',\n        'Moving Average Ratio Analysis',\n        'Volatility Pattern'\n    ),\n    specs=[[{'type': 'bar'}, {'type': 'scatter'}],\n           [{'type': 'scatter'}, {'type': 'histogram'}]]\n)\n\n# Chart 1: Risk distribution\nfig.add_trace(\n    go.Bar(x=picks_df['ticker'], y=picks_df['prob'], \n           name='Short Probability', marker_color='crimson'),\n    row=1, col=1\n)\n\n# Chart 2: RSI vs Probability\nfig.add_trace(\n    go.Scatter(\n        x=picks_df['rsi14'], \n        y=picks_df['prob'],\n        mode='markers+text',\n        text=picks_df['ticker'],\n        textposition='top center',\n        marker=dict(size=12, color=picks_df['prob'], colorscale='Reds'),\n        name='RSI Analysis'\n    ),\n    row=1, col=2\n)\n\n# Chart 3: MA Ratio scatter\nfig.add_trace(\n    go.Scatter(\n        x=picks_df['ma_ratio'], \n        y=picks_df['prob'],\n        mode='markers+text',\n        text=picks_df['ticker'],\n        textposition='top center',\n        marker=dict(size=12, color='blue'),\n        name='MA Ratio'\n    ),\n    row=2, col=1\n)\n\n# Chart 4: Volatility distribution\nfig.add_trace(\n    go.Histogram(x=picks_df['rvol20'], name='Volatility', marker_color='orange'),\n    row=2, col=2\n)\n\nfig.update_layout(\n    height=800, \n    showlegend=False,\n    title_text=\"📊 Multi-Dimensional Short Candidate Analysis\",\n    title_font=dict(size=22, color='#2c3e50')\n)\n\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:49:17.704964Z","iopub.execute_input":"2025-11-09T03:49:17.705853Z","iopub.status.idle":"2025-11-09T03:49:17.893315Z","shell.execute_reply.started":"2025-11-09T03:49:17.705820Z","shell.execute_reply":"2025-11-09T03:49:17.892702Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"820\"\n    src=\"iframe_figures/figure_43.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"# DISPLAY MULTI-AGENT REASONING\n\ndisplay(HTML(\"\"\"\n<div style='background: #f8f9fa; padding: 25px; border-radius: 10px; \n            border-left: 5px solid #667eea; margin: 20px 0;'>\n    <h2 style='color: #2c3e50; margin-top: 0;'>🤖 Multi-Agent Analysis Narrative</h2>\n</div>\n\"\"\"))\n\n# Show narrative from your LangGraph result\nprint(result[\"narrative\"])\nprint(\"\\n\" + \"=\"*70)\n\n# Show Qdrant retrieved context\nprint(\"\\n🔍 VECTOR SEARCH CONTEXT (Top Similar Patterns):\")\nprint(\"=\"*70)\nfor i, hit in enumerate(result[\"retrieved\"][:5], 1):\n    print(f\"\\n{i}. Ticker: {hit['ticker']} | Date: {hit['date']}\")\n    print(f\"   Note: {hit['note'][:100]}...\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:49:21.908729Z","iopub.execute_input":"2025-11-09T03:49:21.909012Z","iopub.status.idle":"2025-11-09T03:49:21.916440Z","shell.execute_reply.started":"2025-11-09T03:49:21.908992Z","shell.execute_reply":"2025-11-09T03:49:21.915733Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<div style='background: #f8f9fa; padding: 25px; border-radius: 10px; \n            border-left: 5px solid #667eea; margin: 20px 0;'>\n    <h2 style='color: #2c3e50; margin-top: 0;'>🤖 Multi-Agent Analysis Narrative</h2>\n</div>\n"},"metadata":{}},{"name":"stdout","text":"Top short candidates (rule-based):\n- CMAX: MA10 < MA20, elevated vol\n- OPTX: MA10 < MA20, elevated vol\n- VLCN: MA10 < MA20, elevated vol\n- ICU: MA10 < MA20, elevated vol\n- BNED: MA10 < MA20, elevated vol\n- MLGO: overbought RSI, elevated vol\n- MEDS: overbought RSI, elevated vol\n- AISP: elevated vol\n- ZNTL: MA10 < MA20, elevated vol\n- BRFH: overbought RSI, elevated vol\n\n======================================================================\n\n🔍 VECTOR SEARCH CONTEXT (Top Similar Patterns):\n======================================================================\n\n1. Ticker: AGRX | Date: 2024-06-25\n   Note: ticker:AGRX; date:2024-06-25; ret1:0.0030; rvol20:0.0281; ma_ratio:0.966; rsi14:34.09...\n\n2. Ticker: AGRX | Date: 2024-06-26\n   Note: ticker:AGRX; date:2024-06-26; ret1:0.5000; rvol20:0.1425; ma_ratio:1.106; rsi14:95.14...\n\n3. Ticker: AGFY | Date: 2024-06-25\n   Note: ticker:AGFY; date:2024-06-25; ret1:-0.0588; rvol20:0.1425; ma_ratio:1.014; rsi14:71.91...\n\n4. Ticker: AGFY | Date: 2024-06-26\n   Note: ticker:AGFY; date:2024-06-26; ret1:-0.0575; rvol20:0.1425; ma_ratio:1.034; rsi14:66.21...\n\n5. Ticker: AGX | Date: 2024-06-26\n   Note: ticker:AGX; date:2024-06-26; ret1:-0.0179; rvol20:0.0269; ma_ratio:1.032; rsi14:68.73...\n======================================================================\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# BACKTEST PERFORMANCE VISUALIZATION (ROBUST VERSION)\n\ntry:\n    # Load from saved file\n    from pathlib import Path\n    ART = Path(\"/kaggle/working/artifacts\")\n    equity_file = ART / \"equity_curve.csv\"\n    \n    if equity_file.exists():\n        equity = pd.read_csv(equity_file)\n        \n        fig = go.Figure()\n        \n        fig.add_trace(go.Scatter(\n            x=list(range(len(equity))),  # Just use index\n            y=equity['equity'],\n            mode='lines',\n            name='Equity',\n            line=dict(color='#667eea', width=3),\n            fill='tozeroy',\n            fillcolor='rgba(102, 126, 234, 0.1)'\n        ))\n        \n        fig.add_hline(y=1.0, line_dash=\"dash\", line_color=\"gray\", \n                      annotation_text=\"Starting Capital\")\n        \n        final_return = (equity['equity'].iloc[-1] - 1) * 100\n        color = 'green' if final_return > 0 else 'red'\n        \n        fig.update_layout(\n            title=f'📈 Backtest Equity Curve (Return: {final_return:+.2f}%)',\n            xaxis_title='Trading Day',\n            yaxis_title='Equity Multiple',\n            height=500,\n            template='plotly_white',\n            font=dict(size=14),\n            title_font=dict(size=20, color=color)\n        )\n        \n        fig.show()\n        \n        print(f\"\\n💰 BACKTEST SUMMARY:\")\n        print(f\"   Initial Capital: 1.00x\")\n        print(f\"   Final Equity: {equity['equity'].iloc[-1]:.3f}x\")\n        print(f\"   Total Return: {final_return:+.2f}%\")\n        print(f\"   Number of Days: {len(equity)}\")\n    else:\n        print(\"⚠️ Backtest file not found - skipping equity curve\")\n        print(\"   This is OK - focus on the ML predictions and agent analysis!\")\n        \nexcept Exception as e:\n    print(f\"⚠️ Backtest visualization skipped: {e}\")\n    print(\"   Not critical - your main results are the top 10 picks!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:49:30.415227Z","iopub.execute_input":"2025-11-09T03:49:30.415786Z","iopub.status.idle":"2025-11-09T03:49:30.469717Z","shell.execute_reply.started":"2025-11-09T03:49:30.415763Z","shell.execute_reply":"2025-11-09T03:49:30.469016Z"}},"outputs":[{"output_type":"display_data","data":{"text/html":"<iframe\n    scrolling=\"no\"\n    width=\"100%\"\n    height=\"520\"\n    src=\"iframe_figures/figure_45.html\"\n    frameborder=\"0\"\n    allowfullscreen\n></iframe>\n"},"metadata":{}},{"name":"stdout","text":"\n💰 BACKTEST SUMMARY:\n   Initial Capital: 1.00x\n   Final Equity: 1979.407x\n   Total Return: +197840.70%\n   Number of Days: 367\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# INTERACTIVE STOCK SELECTION\n\ndef create_stock_explorer():\n    \"\"\"Interactive stock analysis\"\"\"\n    \n    # Stock dropdown\n    stock_select = widgets.Dropdown(\n        options=picks_df['ticker'].tolist(),\n        description='Stock:',\n        style={'description_width': '100px'}\n    )\n    \n    # Analyze button\n    analyze_btn = widgets.Button(\n        description='🔍 Deep Dive Analysis',\n        button_style='success',\n        layout=widgets.Layout(width='250px', height='45px')\n    )\n    \n    # Output\n    output = widgets.Output()\n    \n    def on_click(b):\n        with output:\n            clear_output()\n            ticker = stock_select.value\n            stock_data = picks_df[picks_df['ticker'] == ticker].iloc[0]\n            \n            print(\"=\"*70)\n            print(f\"🎯 DEEP DIVE: {ticker}\")\n            print(\"=\"*70)\n            \n            print(f\"\\n📊 KEY METRICS:\")\n            print(f\"   Short Probability: {stock_data['prob']:.2%}\")\n            print(f\"   RSI (14): {stock_data['rsi14']:.2f}\")\n            print(f\"   MA Ratio (10/20): {stock_data['ma_ratio']:.3f}\")\n            print(f\"   Volatility (20d): {stock_data['rvol20']:.4f}\")\n            \n            print(f\"\\n🚨 RISK SIGNALS:\")\n            if stock_data['rsi14'] > 70:\n                print(\"   ⚠️ OVERBOUGHT - RSI above 70\")\n            if stock_data['ma_ratio'] < 1.0:\n                print(\"   ⚠️ DOWNTREND - MA10 below MA20\")\n            if stock_data['rvol20'] > picks_df['rvol20'].median():\n                print(\"   ⚠️ HIGH VOLATILITY - Above median\")\n            \n            print(f\"\\n💡 RECOMMENDATION:\")\n            if stock_data['prob'] > 0.5:\n                print(\"   🔴 STRONG SHORT - High probability candidate\")\n            elif stock_data['prob'] > 0.4:\n                print(\"   🟡 MODERATE SHORT - Consider position sizing\")\n            else:\n                print(\"   🟢 WEAK SHORT - Monitor but don't prioritize\")\n            \n            print(\"=\"*70)\n    \n    analyze_btn.on_click(on_click)\n    \n    display(widgets.VBox([\n        widgets.HTML(\"<h3>🔬 Interactive Stock Explorer</h3>\"),\n        stock_select,\n        analyze_btn,\n        output\n    ]))\n\ncreate_stock_explorer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:49:37.057132Z","iopub.execute_input":"2025-11-09T03:49:37.057432Z","iopub.status.idle":"2025-11-09T03:49:37.075901Z","shell.execute_reply.started":"2025-11-09T03:49:37.057412Z","shell.execute_reply":"2025-11-09T03:49:37.075181Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<h3>🔬 Interactive Stock Explorer</h3>'), Dropdown(description='Stock:', options=('C…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f5fc3f267cb49baa3e8c9c31aabc601"}},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"# COMPREHENSIVE SUMMARY TABLE\n\ndisplay(HTML(\"\"\"\n<h2 style='color: #2c3e50; border-bottom: 3px solid #667eea; padding-bottom: 10px;'>\n    📊 Complete Analysis Summary\n</h2>\n\"\"\"))\n\nsummary_data = {\n    'Metric': [\n        'Total Stocks Analyzed',\n        'Date of Analysis',\n        'Short Candidates Identified',\n        'Average Short Probability',\n        'Highest Risk Stock',\n        'Model AUROC',\n        'Model Precision@10',\n        'Backtest Return',\n        'Vector DB Size',\n        'Qdrant Retrievals'\n    ],\n    'Value': [\n        f\"{len(result['today_scores'])} stocks\",\n        f\"{result['today_scores']['date'].iloc[0]}\",\n        f\"{len(result['picks'])} stocks\",\n        f\"{result['picks']['prob'].mean():.2%}\",\n        f\"{result['picks'].iloc[0]['ticker']} ({result['picks'].iloc[0]['prob']:.2%})\",\n        f\"{metrics.get('auroc', 0):.3f}\",\n        f\"{metrics.get('precision@10', 0):.2f}\",\n        f\"{final_return:+.2f}%\" if 'final_return' in dir() else 'N/A',\n        f\"{len(result['retrieved'])} vectors\",\n        f\"{len(result['retrieved'])} similar patterns found\"\n    ]\n}\n\nsummary_df = pd.DataFrame(summary_data)\n\n# Style the dataframe\nstyled_summary = summary_df.style.set_properties(**{\n    'text-align': 'left',\n    'font-size': '14px',\n    'border': '1px solid #ddd'\n}).set_table_styles([\n    {'selector': 'th', 'props': [('background-color', '#667eea'), \n                                   ('color', 'white'), \n                                   ('font-weight', 'bold'),\n                                   ('text-align', 'left'),\n                                   ('padding', '12px')]},\n    {'selector': 'td', 'props': [('padding', '10px')]}\n])\n\ndisplay(styled_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:49:49.342136Z","iopub.execute_input":"2025-11-09T03:49:49.342457Z","iopub.status.idle":"2025-11-09T03:49:49.429762Z","shell.execute_reply.started":"2025-11-09T03:49:49.342434Z","shell.execute_reply":"2025-11-09T03:49:49.429036Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<h2 style='color: #2c3e50; border-bottom: 3px solid #667eea; padding-bottom: 10px;'>\n    📊 Complete Analysis Summary\n</h2>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7fe43440e650>","text/html":"<style type=\"text/css\">\n#T_46c36 th {\n  background-color: #667eea;\n  color: white;\n  font-weight: bold;\n  text-align: left;\n  padding: 12px;\n}\n#T_46c36 td {\n  padding: 10px;\n}\n#T_46c36_row0_col0, #T_46c36_row0_col1, #T_46c36_row1_col0, #T_46c36_row1_col1, #T_46c36_row2_col0, #T_46c36_row2_col1, #T_46c36_row3_col0, #T_46c36_row3_col1, #T_46c36_row4_col0, #T_46c36_row4_col1, #T_46c36_row5_col0, #T_46c36_row5_col1, #T_46c36_row6_col0, #T_46c36_row6_col1, #T_46c36_row7_col0, #T_46c36_row7_col1, #T_46c36_row8_col0, #T_46c36_row8_col1, #T_46c36_row9_col0, #T_46c36_row9_col1 {\n  text-align: left;\n  font-size: 14px;\n  border: 1px solid #ddd;\n}\n</style>\n<table id=\"T_46c36\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_46c36_level0_col0\" class=\"col_heading level0 col0\" >Metric</th>\n      <th id=\"T_46c36_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_46c36_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_46c36_row0_col0\" class=\"data row0 col0\" >Total Stocks Analyzed</td>\n      <td id=\"T_46c36_row0_col1\" class=\"data row0 col1\" >3231 stocks</td>\n    </tr>\n    <tr>\n      <th id=\"T_46c36_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_46c36_row1_col0\" class=\"data row1 col0\" >Date of Analysis</td>\n      <td id=\"T_46c36_row1_col1\" class=\"data row1 col1\" >2024-06-26 00:00:00</td>\n    </tr>\n    <tr>\n      <th id=\"T_46c36_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_46c36_row2_col0\" class=\"data row2 col0\" >Short Candidates Identified</td>\n      <td id=\"T_46c36_row2_col1\" class=\"data row2 col1\" >10 stocks</td>\n    </tr>\n    <tr>\n      <th id=\"T_46c36_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_46c36_row3_col0\" class=\"data row3 col0\" >Average Short Probability</td>\n      <td id=\"T_46c36_row3_col1\" class=\"data row3 col1\" >51.10%</td>\n    </tr>\n    <tr>\n      <th id=\"T_46c36_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_46c36_row4_col0\" class=\"data row4 col0\" >Highest Risk Stock</td>\n      <td id=\"T_46c36_row4_col1\" class=\"data row4 col1\" >CMAX (59.83%)</td>\n    </tr>\n    <tr>\n      <th id=\"T_46c36_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_46c36_row5_col0\" class=\"data row5 col0\" >Model AUROC</td>\n      <td id=\"T_46c36_row5_col1\" class=\"data row5 col1\" >0.652</td>\n    </tr>\n    <tr>\n      <th id=\"T_46c36_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n      <td id=\"T_46c36_row6_col0\" class=\"data row6 col0\" >Model Precision@10</td>\n      <td id=\"T_46c36_row6_col1\" class=\"data row6 col1\" >0.60</td>\n    </tr>\n    <tr>\n      <th id=\"T_46c36_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n      <td id=\"T_46c36_row7_col0\" class=\"data row7 col0\" >Backtest Return</td>\n      <td id=\"T_46c36_row7_col1\" class=\"data row7 col1\" >+197840.70%</td>\n    </tr>\n    <tr>\n      <th id=\"T_46c36_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n      <td id=\"T_46c36_row8_col0\" class=\"data row8 col0\" >Vector DB Size</td>\n      <td id=\"T_46c36_row8_col1\" class=\"data row8 col1\" >10 vectors</td>\n    </tr>\n    <tr>\n      <th id=\"T_46c36_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n      <td id=\"T_46c36_row9_col0\" class=\"data row9 col0\" >Qdrant Retrievals</td>\n      <td id=\"T_46c36_row9_col1\" class=\"data row9 col1\" >10 similar patterns found</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"# EXPORT RESULTS FOR JUDGES\n\nfrom pathlib import Path\nART = Path(\"/kaggle/working/artifacts\")\n\nprint(\"📦 EXPORTING RESULTS...\")\nprint(\"=\"*70)\n\n# What's been saved\nfiles = list(ART.iterdir())\nprint(f\"\\n✅ {len(files)} files created in /kaggle/working/artifacts/:\")\nfor f in files:\n    size_kb = f.stat().st_size / 1024\n    print(f\"   • {f.name:30} ({size_kb:,.1f} KB)\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"📥 TO DOWNLOAD:\")\nprint(\"   1. Click 'Save Version' (top right)\")\nprint(\"   2. Go to 'Output' tab\")\nprint(\"   3. Download all artifacts\")\nprint(\"=\"*70)\n\n\nwith open(ART / \"README.txt\", \"w\") as f:\n    f.write(readme)\n\nprint(\"\\n✅ README.txt created!\")\nprint(\"\\n🎉 ALL RESULTS EXPORTED AND READY!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T03:51:00.999831Z","iopub.execute_input":"2025-11-09T03:51:01.000123Z","iopub.status.idle":"2025-11-09T03:51:01.007688Z","shell.execute_reply.started":"2025-11-09T03:51:01.000103Z","shell.execute_reply":"2025-11-09T03:51:01.007013Z"}},"outputs":[{"name":"stdout","text":"📦 EXPORTING RESULTS...\n======================================================================\n\n✅ 7 files created in /kaggle/working/artifacts/:\n   • picks.csv                      (1.1 KB)\n   • narrative.txt                  (0.4 KB)\n   • README.txt                     (0.9 KB)\n   • today_scores.csv               (358.8 KB)\n   • model_pipeline.pkl             (29.2 KB)\n   • equity_curve.csv               (7.9 KB)\n   • metrics.csv                    (0.1 KB)\n\n======================================================================\n📥 TO DOWNLOAD:\n   1. Click 'Save Version' (top right)\n   2. Go to 'Output' tab\n   3. Download all artifacts\n======================================================================\n\n✅ README.txt created!\n\n🎉 ALL RESULTS EXPORTED AND READY!\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}